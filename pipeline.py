"""
News Structurizer Pipeline
Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.

–ó–∞–ø—É—Å–∫: streamlit run pipeline.py
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
BASE_DIR = Path(__file__).parent
sys.path.insert(0, str(BASE_DIR / "topicsegmenter"))

import re
import inspect
from typing import List, Dict, Any

import torch
import streamlit as st
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    pipeline as hf_pipeline,
)


# ============ –£—Ç–∏–ª–∏—Ç—ã ============

def normalize_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (—Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å topicsegmenter/src/utils.py)."""
    text = text.lower()
    text = re.sub(r'[^a-z–∞-—è—ë\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# ============ –ú–æ–¥–µ–ª–∏ ============

class NewsSegmenter:
    """–°–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π - —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ—Ç–æ–∫ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏."""

    def __init__(self, model_path: str):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        self._accepts_token_type_ids = self._check_token_type_ids()

    def _check_token_type_ids(self) -> bool:
        try:
            return "token_type_ids" in inspect.signature(self.model.forward).parameters
        except (TypeError, ValueError):
            return False

    def _predict_batch(self, pairs: List[tuple]) -> List[float]:
        if not pairs:
            return []

        batch_size = 32
        all_probs = []

        for i in range(0, len(pairs), batch_size):
            batch = pairs[i:i+batch_size]
            lefts = [normalize_text(p[0]) for p in batch]
            rights = [normalize_text(p[1]) for p in batch]

            inputs = self.tokenizer(
                lefts, rights,
                add_special_tokens=True,
                max_length=128,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )

            if not self._accepts_token_type_ids and "token_type_ids" in inputs:
                inputs.pop("token_type_ids")

            inputs = inputs.to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.softmax(outputs.logits, dim=1)
                all_probs.extend(probs[:, 1].cpu().tolist())

        return all_probs

    def segment(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏."""
        words = text.split()

        MIN_LEN = 10
        CONFIRM_THR = 0.8

        scan_indices = list(range(MIN_LEN, len(words) - MIN_LEN))
        if not scan_indices:
            return [text]

        candidates = []
        for i in scan_indices:
            ctx_left = " ".join(words[max(0, i-50):i])
            ctx_right = " ".join(words[i:min(len(words), i+50)])
            candidates.append((ctx_left, ctx_right))

        probs = self._predict_batch(candidates)

        split_indices = [0]
        i = 0
        while i < len(probs):
            prob = probs[i]
            idx = scan_indices[i]

            is_peak = True
            if i > 0 and probs[i-1] >= prob:
                is_peak = False
            if i < len(probs) - 1 and probs[i+1] > prob:
                is_peak = False

            if is_peak and prob > CONFIRM_THR:
                if idx - split_indices[-1] >= MIN_LEN:
                    split_indices.append(idx)
                    while i < len(scan_indices) and scan_indices[i] < idx + MIN_LEN:
                        i += 1
                    continue
            i += 1

        split_indices.append(len(words))

        segments = []
        for k in range(len(split_indices) - 1):
            segments.append(" ".join(words[split_indices[k]:split_indices[k+1]]))

        return segments


class NewsClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç topic –∏ scale."""

    def __init__(self, topic_model_path: str, scale_model_path: str):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.topic_tokenizer = AutoTokenizer.from_pretrained(topic_model_path)
        self.topic_model = AutoModelForSequenceClassification.from_pretrained(topic_model_path)
        self.topic_model.to(self.device)
        self.topic_model.eval()

        self.scale_tokenizer = AutoTokenizer.from_pretrained(scale_model_path)
        self.scale_model = AutoModelForSequenceClassification.from_pretrained(scale_model_path)
        self.scale_model.to(self.device)
        self.scale_model.eval()

    def _predict(self, text: str, tokenizer, model, max_len: int = 256) -> Dict[str, Any]:
        inputs = tokenizer(
            text,
            truncation=True,
            max_length=max_len,
            return_tensors="pt",
        ).to(self.device)

        with torch.no_grad():
            logits = model(**inputs).logits
            probs = torch.softmax(logits, dim=-1)[0]
            pred_idx = int(torch.argmax(probs).item())

        label = model.config.id2label[pred_idx]
        return {"label": label, "confidence": float(probs[pred_idx])}

    def classify(self, text: str) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ topic –∏ scale."""
        topic_result = self._predict(text, self.topic_tokenizer, self.topic_model)
        scale_result = self._predict(text, self.scale_tokenizer, self.scale_model)

        return {
            "topic": topic_result["label"],
            "topic_confidence": topic_result["confidence"],
            "scale": scale_result["label"],
            "scale_confidence": scale_result["confidence"],
        }


class NewsAttributeGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∞—Ç—Ä–∏–±—É—Ç–æ–≤ - –∏–∑–≤–ª–µ–∫–∞–µ—Ç title, key_events, location, key_names."""

    def __init__(self, model_path: str):
        device = 0 if torch.cuda.is_available() else -1
        self.generator = hf_pipeline(
            "text2text-generation",
            model=model_path,
            tokenizer=model_path,
            device=device
        )

        self.tasks = {
            "title": "–∑–∞–≥–æ–ª–æ–≤–æ–∫: ",
            "key_events": "—Å–æ–±—ã—Ç–∏–µ: ",
            "location": "–ª–æ–∫–∞—Ü–∏—è: ",
            "key_names": "–∏–º–µ–Ω–∞: "
        }

    def generate(self, text: str) -> Dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è —Ç–µ–∫—Å—Ç–∞."""
        results = {}

        for key, prefix in self.tasks.items():
            input_text = prefix + text
            output = self.generator(
                input_text,
                max_length=200,
                num_beams=4,
                early_stopping=True
            )[0]['generated_text']
            results[key] = output

        return results


# ============ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ============

@st.cache_resource
def load_segmenter():
    model_path = str(BASE_DIR / "topicsegmenter" / "checkpoints" / "best_model")
    return NewsSegmenter(model_path)


@st.cache_resource
def load_classifier():
    topic_path = str(BASE_DIR / "classification" / "models_out_sbert_large_nlu_ru" / "topic" / "best")
    scale_path = str(BASE_DIR / "classification" / "models_out_sbert_large_nlu_ru" / "scale" / "best")
    return NewsClassifier(topic_path, scale_path)


@st.cache_resource
def load_generator():
    model_path = str(BASE_DIR / "rut5_extractor" / "final_model")
    return NewsAttributeGenerator(model_path)


# ============ –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω ============

def process_news_stream(text: str) -> List[Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Ç–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."""
    segmenter = load_segmenter()
    classifier = load_classifier()
    generator = load_generator()

    # 1. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
    segments = segmenter.segment(text)

    results = []
    for segment in segments:
        # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        classification = classifier.classify(segment)

        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        attributes = generator.generate(segment)

        results.append({
            "text": segment,
            "topic": classification["topic"],
            "topic_confidence": classification["topic_confidence"],
            "scale": classification["scale"],
            "scale_confidence": classification["scale_confidence"],
            "title": attributes["title"],
            "key_events": attributes["key_events"],
            "location": attributes["location"],
            "key_names": attributes["key_names"],
        })

    return results


# ============ Streamlit UI ============

def main():
    st.set_page_config(
        page_title="News Structurizer",
        page_icon="üì∞",
        layout="wide"
    )

    st.title("üì∞ News Structurizer")
    st.markdown("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø–æ—Ç–æ–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")

    # Sidebar —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    with st.sidebar:
        st.header("–û –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏")
        st.markdown("""
        **–ü–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏:**
        1. **–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è** - —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        2. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è** - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º—ã –∏ –º–∞—Å—à—Ç–∞–±–∞
        3. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è** - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞, —Å–æ–±—ã—Ç–∏–π, –ª–æ–∫–∞—Ü–∏–∏, –∏–º—ë–Ω
        """)

        st.header("–ú–æ–¥–µ–ª–∏")
        st.markdown("""
        - **Segmenter**: RuBERT (DeepPavlov)
        - **Classifier**: SBERT Large NLU RU
        - **Generator**: ruT5-base
        """)

    # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    text_input = st.text_area(
        "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π:",
        height=200,
        placeholder="–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ —Å–ø–ª–æ—à–Ω–æ–π —Ç–µ–∫—Å—Ç —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏..."
    )

    col1, col2 = st.columns([1, 5])
    with col1:
        process_btn = st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å", type="primary")

    if process_btn and text_input.strip():
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π..."):
            # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
            load_segmenter()
            load_classifier()
            load_generator()

        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞..."):
            results = process_news_stream(text_input)

        st.success(f"–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(results)}")

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        for i, news in enumerate(results, 1):
            with st.expander(f"üì∞ –ù–æ–≤–æ—Å—Ç—å {i}: {news['title'][:80]}...", expanded=(i == 1)):
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("–¢–µ–º–∞", news["topic"], f"{news['topic_confidence']:.1%}")
                with col2:
                    st.metric("–ú–∞—Å—à—Ç–∞–±", news["scale"], f"{news['scale_confidence']:.1%}")

                st.divider()

                # –ê—Ç—Ä–∏–±—É—Ç—ã
                st.markdown(f"**–ó–∞–≥–æ–ª–æ–≤–æ–∫:** {news['title']}")
                st.markdown(f"**–ö–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è:** {news['key_events']}")
                st.markdown(f"**–õ–æ–∫–∞—Ü–∏—è:** {news['location']}")
                st.markdown(f"**–ö–ª—é—á–µ–≤—ã–µ –∏–º–µ–Ω–∞:** {news['key_names']}")

                st.divider()

                # –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
                with st.container():
                    st.markdown("**–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:**")
                    st.text(news["text"][:500] + ("..." if len(news["text"]) > 500 else ""))

    elif process_btn:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")


if __name__ == "__main__":
    main()

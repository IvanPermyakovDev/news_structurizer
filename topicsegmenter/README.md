# Отчет по текущему решению Topic Segmenter (Kazakh)

## 1) Задача и целевой сценарий

Цель проекта — находить границы между "новостями/темами" внутри длинного потока текста на казахском языке, похожего на результат распознавания речи (ASR): без пунктуации, с ошибками в словах, со сломанными границами предложений и возможными "склейками" фраз.

Практический формат задачи здесь сведен к бинарной проверке: "есть ли разрыв темы **в конкретной позиции** между левым и правым контекстом".

Результат работы системы в инференсе — разбиение длинной строки на список сегментов (каждый сегмент соответствует одной теме/новости).

## 2) Общая идея решения

Решение построено вокруг модели семейства RoBERTa для казахского языка, обученной как бинарный классификатор:

- На вход подаются две строки: `context_left` и `context_right`.
- Модель возвращает вероятность класса 1 — "между левым и правым контекстом проходит граница темы".

Далее, для разметки "реального" длинного текста используется сканирование возможных границ по словам: для каждой позиции строятся левый и правый контекст, прогоняются через модель, после чего по локальным максимумам вероятности выбираются места разрезов.

## 3) Структура репозитория (важные файлы)

- `src/config.py` — все ключевые параметры (пути, гиперпараметры обучения, вероятности аугментаций, устройство).
- `src/dataset.py` — датасет для обучения/валидации, а также логика синтетического усложнения данных (mixing + jittering + текстовые аугментации).
- `src/augmentations.py` — "ASR-подобные" искажения текста, адаптированные под казахский язык.
- `src/utils.py` — фиксация seed и настройки детерминизма.
 - `train.py` — обучение, логирование в TensorBoard, сохранение лучшей модели (по F1 или по concat-валидации, см. `Config.select_best_by`).
- `inference.py` — сегментация длинного текста (поиск разрывов в потоке).
- `data/split_dataset.py` — разбиение исходного файла на `data/train.jsonl` и `data/val.jsonl`.

## 4) Формат данных и статистика

### 4.1. Формат одного примера

Каждый пример — это объект с тремя ключами:

- `context_left`: текст слева от предполагаемой границы
- `context_right`: текст справа от предполагаемой границы
- `label`: 1 если между ними реальный разрыв темы, иначе 0

Важно: в обучении датасет может "переопределять" место разреза и метку через jitter/mixing (см. раздел 5).

### 4.2. Наборы и размерности

По текущим файлам:

- `data/train.jsonl`: 9024 примеров
- `data/val.jsonl`: 1003 примеров
- `dataset_segmentor_kz.jsonl`: 10027 примеров (после фильтрации некорректных записей)

Модель при токенизации использует ограничение `max_len` (по умолчанию 128 токенов), поэтому фактически "видимый" контекст может быть меньше исходного текста по словам.

### 4.3. Примеры данных (без кода)

Пример с `label = 1` (разрыв темы):

- `context_left`: фрагмент про президента и государственные награды спортсменам
- `context_right`: фрагмент про пожар в Кокшетау ("көкшетау қаласында түнде қатты өрт болды…")
- смысл: модель должна уверенно сработать, потому что "переход" резкий и тематически не связан

Пример с `label = 0` (нет разрыва):

- `context_left`: фрагмент про футбольный матч и победу команды
- `context_right`: продолжение про позицию в турнирной таблице и следующий матч
- смысл: это одна тема, разрез искусственный и не должен восприниматься как граница новостей

## 5) Подготовка данных в `SegmentationDataset`

В `src/dataset.py` при обучении применяется три уровня усложнения данных.

### 5.1. Synthetic Topic Mixing (смешивание тем)

С вероятностью `mix_prob` левый контекст берется из текущего примера, а правый — из другого случайного примера (как "чужое продолжение").

Эффект:

- получается гарантированный тематический разрыв
- метка принудительно становится `label = 1`
- модель видит больше "контрастных" разрывов и учится не зависеть от конкретных шаблонов исходного датасета

Пример:

- слева: "бүгін астанада ауа райы бұлтты болады…"
- справа: "халықаралық валюта қоры инфляция болжамын жариялады…"
- ожидаемая метка: разрыв есть (1), потому что темы не связаны

### 5.2. Jittering (сдвиг границы — hard negatives)

Если исходный пример имеет `label = 1`, то при обучении:

- с вероятностью `jitter_prob` граница сохраняется как есть (остается `label = 1`)
- иначе граница сдвигается на случайное число слов в диапазоне до `max_jitter_shift`, а метка становится `label = 0`

Зачем это нужно:

- модель должна не просто понимать "что где-то рядом есть разрыв", а уметь **локализовать** его
- сдвинутые разрезы около реальной границы становятся сложными отрицательными примерами: контекст уже "подпорчен" соседней темой, но это еще не "правильная" позиция разреза

Пример (идея):

- истинная граница: …"матч үш екі есебімен аяқталды" | "қаржы министрлігі ұсынды…"
- jitter-граница (на 2 слова правее): …"үш екі қаржы" | "министрлігі ұсынды…"
- метка для jitter-границы: 0, потому что разрез поставлен неверно, несмотря на близость к реальной границе

Если исходный пример имеет `label = 0`, то при обучении разрез выбирается случайно внутри полного текста (не у самого края), чтобы получать разнообразные "обычные" отрицательные примеры.

### 5.3. Текстовые аугментации под ASR (казахский язык)

С вероятностью `aug_prob` к каждому из контекстов независимо применяется пайплайн искажений из `src/augmentations.py`. Общая идея — сделать текст похожим на "грязный" ASR с учётом специфики казахского языка:

**Фонетические замены (специфичные для казахского):**
- Путаница между қ↔к, ғ↔г, ң↔н, ө↔о, ұ↔ү, ы↔і, һ↔х, ә↔а
- Пример: "қала" → "кала", "өңір" → "онір"

**Агглютинативные ошибки:**
- Отделение суффиксов от корня: "парламенттің" → "парламент тің"
- Выпадение окончаний: "жұмыстарын" → "жұмыстары"
- Склейка слов с частицами (мен, бен, пен, да, де): "бұл жағдайда" → "бұлжағдайда"

**Замены созвучных слов (омофоны):**
- "қала" ↔ "қара" (город ↔ смотри/чёрный)
- "келді" ↔ "берді" (пришёл ↔ дал)

**Повторы и слова-паразиты:**
- Заикание: "мемлекеттік" → "ме мемлекеттік"
- Повтор слов: "жат" → "жат жат"
- Вставка филлеров: "іммм", "жаңағы", "негізі", "былайша айтқанда"

**Вставка якорных фраз (для правого контекста):**
- "ал енді келесі тақырып"
- "спорт жаңалықтарына тоқталсақ"
- "ауа райы болжамына көшсек"

## 6) Модель и представление входа

### 6.1. Базовая модель

Используется `kz-transformers/kaz-roberta-conversational` как основа и головка бинарной классификации (2 класса).

### 6.2. Что именно классифицируется

Классифицируется не "текст целиком", а **граница между двумя частями**. Вход формируется как пара последовательностей (левый и правый контекст), что позволяет модели сравнивать смысл и связность "до" и "после".

### 6.3. Ограничение по длине

В токенизаторе выставлен `max_len = 128` с усечением. Это означает:

- длинные контексты "обрезаются"
- модель в большей степени опирается на локальные признаки вокруг предполагаемой границы, чем на весь исходный фрагмент

## 7) Обучение и оценка качества

В `train.py`:

- логирование ведется в `runs/experiment_v1` (TensorBoard)
- лучшая модель сохраняется по `Config.select_best_by`:
  - `"f1"` — классический F1 на `data/val.jsonl`
  - `"concat"` — concat-валидация: склеиваем 2 текста из `concat_val_path` и оцениваем, насколько хорошо найден разрыв
- метрики: Accuracy, Precision, Recall, F1 (binary)

Ключевые настройки по умолчанию из `src/config.py`:

- batch size: 16
- epochs: 100
- learning rate: 5e-5, weight decay: 0.01
- warmup: 200 шагов

## 8) Инференс: как из вероятностей получаются сегменты

Класс `NewsSegmenter` в `inference.py` решает задачу "нарезки" так:

1) Текст разбивается на слова.
2) Дальше рассматриваются позиции между словами как кандидаты на границу.
3) Для кандидата строятся:
   - левый контекст: до позиции (с ограничением окна примерно до 50 слов)
   - правый контекст: после позиции (аналогично)
4) Модель оценивает вероятность разрыва.
5) По тексту идет грубый проход с шагом (по умолчанию 5 слов). Если вероятность превысила небольшой "триггер":
   - запускается локальный поиск пика в окрестности (несколько слов вокруг)
6) Если найденный пик выше порога подтверждения, то граница фиксируется, и алгоритм перескакивает вперед, чтобы не резать слишком часто.

Параметры инференса (в текущем виде "зашиты" в `segment_text`):

- шаг грубого поиска: 5 слов
- порог "подозрения": 0.05
- порог подтверждения: 0.8
- минимальная длина сегмента: 5 слов

### Пример ожидаемого результата сегментации (без кода)

Вход (единый поток слов, 3 темы):

1) "бүгін астанада ауа райы бұлтты болады … жел орташа"
2) "хоккей клубы барыс жеңіске жетті … шешуші шайбаны соңғы минутта салды …"
3) "қытайда жаңа зымыран сәтті ұшырылды … байланыс спутнигін орбитаға шығарды …"

Выход (3 сегмента):

- Сегмент 1: весь блок про погоду
- Сегмент 2: весь блок про хоккей
- Сегмент 3: весь блок про запуск ракеты

## 9) Результаты

F1: 0.8903 | P: 0.8793 | R: 0.9015
